{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1 Introduction Traitement Automatique du Langage Naturel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quoi sert le Traitement Automatique du Langage Naturel (TAL) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. A quoi sert le Traitement Automatique du Langage Naturel (TAL) ?\n",
    "Le traitement automatique du Langage Naturel est un des domaines de recherche les plus actifs en science des données actuellement. C’est un domaine à l’intersection du Machine Learning et de la linguistique. Il a pour but d’extraire des informations et une signification d’un contenu textuel.\n",
    "\n",
    "Le Traitement Automatique du Langage naturel (TAL) ou Natural Language Processing (NLP) en anglais trouve de nombreuses applications dans la vie de tous les jours:\n",
    "\n",
    "traduction de texte (DeepL par exemple)\n",
    "correcteur orthographique\n",
    "résumé automatique d’un contenu\n",
    "synthèse vocale\n",
    "classification de texte\n",
    "analyse d’opinion/sentiment\n",
    "prédiction du prochain mot sur smartphone\n",
    "extraction des entités nommées depuis un texte\n",
    "…\n",
    "La plupart des ressources disponibles actuellement sont en anglais, ce qui implique que la plupart des modèles pré-entraînés sont également spécifiques à la langue anglaise. Cependant, il existe des librairies et des outils en français pour accomplir les tâches mentionnées ci-dessus. Nous allons les voir dans cet article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les grands principes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Les grands principes\n",
    "Le TAL (traitement automatique du langage) est généralement composé de deux à trois grandes étapes:\n",
    "\n",
    "Pré-traitement : une étape qui cherche à standardiser du texte afin de rendre son usage plus facile\n",
    "Représentation du texte comme un vecteur : Cette étape peut être effectuée via des techniques de sac de mots (Bag of Words) ou Term Frequency-Inverse Document Frequency (Tf-IdF). On peut également apprendre des représentations vectorielles (embedding) par apprentissage profond.\n",
    "Classification, trouver la phrase la plus similaire… (optionnel).\n",
    "Dans cet article, nous allons couvrir les tâches de TAL les plus communes pour lesquelles des outils spécifiques au français existent.\n",
    "\n",
    "Nous utiliserons principalement SpaCy. SpaCy est une jeune librairie (2015) qui offre des modèles pré-entraînés pour diverses applications, y compris la reconnaissance d’entités nommées. SpaCy est la principale alternative à NLTK (Natural Language Tool Kit), la librairie historique pour le TAL avec Python, et propose de nombreuses innovations et options de visualisation qui sont très intéressantes.\n",
    "\n",
    "Après avoir installé la librairie SpaCy (pip install spacy), il faut télécharger les modèles français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle est un réseau convolutionnel entraîne sur deux corpus, WikiNER et Sequoia, ce qui représente de gros volumes de données en français (typiquement plusieurs dizaines de Go).\n",
    "\n",
    "Dans un notebook Jupyter, on peut alors importer SpaCy et charger le modèle français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:34:01.359784Z",
     "start_time": "2023-04-03T08:34:01.332031Z"
    }
   },
   "outputs": [],
   "source": [
    "#piop install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:34:11.780881Z",
     "start_time": "2023-04-03T08:34:01.359784Z"
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.cli.download('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:34:13.371736Z",
     "start_time": "2023-04-03T08:34:11.780881Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:34:13.387738Z",
     "start_time": "2023-04-03T08:34:13.371736Z"
    }
   },
   "outputs": [],
   "source": [
    "test = \"Bouygues a eu une coupure de réseau à Marseille.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation cherche à transformer un texte en une série de tokens individuels. Dans l’idée, chaque token représente un mot, et identifier des mots semble être une tâche relativement simple. Mais comment gérer en français des exemples tels que: « J’ai froid ». Il faut que le modèle de tokenisation sépare le « J' » comme étant un premier mot.\n",
    "\n",
    "SpaCy offre une fonctionnalité de tokenisation en utilisant la fonction nlp. Cette fonction est le point d’entrée vers toutes les fonctionnalités de SpaCy. Il sert à représenter le texte sous une forme interprétable par la librairie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:35:43.426790Z",
     "start_time": "2023-04-03T08:35:43.410446Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_token(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [X.text for X in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:36:00.146847Z",
     "start_time": "2023-04-03T08:36:00.122840Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bouygues a eu une coupure de réseau à Marseille.\n",
      "['Bouygues', 'a', 'eu', 'une', 'coupure', 'de', 'réseau', 'à', 'Marseille', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(return_token(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlever les mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certains mots se retrouvent très fréquemment dans la langue française. En anglais, on les appelle les « stop words ». Ces mots, bien souvent, n’apportent pas d’information dans les tâches suivantes. Lorsque l’on effectue par exemple une classification par la méthode Tf-IdF, on souhaite limiter la quantité de mots dans les données d’entraînement.\n",
    "\n",
    "Les « stop words » sont établis comme des listes de mots. Ces listes sont généralement disponibles dans une librairie appelée NLTK (Natural Language Tool Kit), et dans beaucoup de langues différentes. On accède aux listes en français de cette manière:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour filtrer le contenu de la phrase, on enlève tous les mots présents dans cette liste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.0rc2\n"
     ]
    }
   ],
   "source": [
    "!python --version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:39:59.750300Z",
     "start_time": "2023-04-03T08:39:59.717774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/constantinbogdanas/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stopWords = set(stopwords.words('french'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:40:09.080563Z",
     "start_time": "2023-04-03T08:40:09.039793Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('french'))\n",
    "\n",
    "clean_words = []\n",
    "for token in return_token(test):\n",
    "    if token not in stopWords:\n",
    "        clean_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:40:16.911996Z",
     "start_time": "2023-04-03T08:40:16.904043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aurai', 'les', 'aurait', 'eussiez', 'à', 'suis', 'une', 'furent', 'ayante', 'aurez', 'ayez', 'eus', 'étante', 'aurions', 'fûmes', 'étais', 'sur', 'serai', 'mes', 'serait', 'fût', 'fussent', 'ont', 'ai', 'aviez', 'son', 'étant', 'ne', 's', 'sa', 'notre', 'as', 'y', 'est', 'votre', 'eu', 'serais', 'en', 'étantes', 'eusse', 'eussions', 'étaient', 'pas', 'le', 'il', 'ayantes', 'fussiez', 'soit', 'eûmes', 'leur', 'm', 'étiez', 'c', 'toi', 'fusses', 't', 'par', 'aient', 'eurent', 'se', 'sommes', 'aie', 'été', 'seront', 'avions', 'tu', 'ce', 'fussions', 'sera', 'ton', 'ayons', 'moi', 'auront', 'nos', 'étants', 'ait', 'ils', 'soyons', 'l', 'que', 'avez', 'auriez', 'eût', 'avons', 'ma', 'fusse', 'eux', 'mon', 'soient', 'avec', 'fûtes', 'étions', 'n', 'me', 'aies', 'es', 'ses', 'un', 'sont', 'des', 'ces', 'serez', 'aura', 'pour', 'dans', 'auras', 'étées', 'eut', 'eûtes', 'était', 'sois', 'êtes', 'qui', 'eues', 'même', 'vos', 'aurais', 'avais', 'nous', 'du', 'tes', 'ta', 'lui', 'd', 'ayants', 'fut', 'étée', 'seras', 'aux', 'et', 'serions', 'vous', 'la', 'elle', 'ayant', 'seriez', 'soyez', 'avait', 'eussent', 'on', 'avaient', 'étés', 'ou', 'te', 'eue', 'seraient', 'eusses', 'au', 'serons', 'je', 'qu', 'mais', 'auraient', 'aurons', 'fus', 'de', 'j'}\n"
     ]
    }
   ],
   "source": [
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:15:07.134574Z",
     "start_time": "2023-04-02T22:15:07.110513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bouygues', 'a', 'coupure', 'réseau', 'Marseille', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation de phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également appliquer une tokenisation par phrase afin d’identifier les différentes phrases d’un texte. Cette étape peut à nouveau sembler facile, puisque a priori, il suffit de couper chaque phrase lorsqu’un point est rencontré (ou un point d’exclamation ou d’interrogation).\n",
    "\n",
    "Mais que se passerait-t-il dans ce cas-là?\n",
    "\n",
    "Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.\n",
    "\n",
    "Il faut donc une compréhension du contexte afin d’effectuer une bonne tokenisation par phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:15:09.746152Z",
     "start_time": "2023-04-02T22:15:09.722270Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_token_sent(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [X.text for X in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:15:10.826575Z",
     "start_time": "2023-04-02T22:15:10.789474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bouygues a eu une coupure de réseau à Marseille.',\n",
       " 'La panne a affecté 300.000 utilisateurs.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_token_sent(\"Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation et Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le stemming et la lemmatisation sont des méthodes utilisées par les moteurs de recherche et les chatbots pour analyser le sens d'un mot. La radicalisation utilise la racine du mot, tandis que la lemmatisation utilise le contexte dans lequel le mot est utilisé. Nous reviendrons plus tard sur des explications et des exemples plus détaillés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le stemming consiste à réduire un mot dans sa forme « racine ». Le but du stemming est de regrouper de nombreuses variantes d’un mot comme un seul et même mot. Par exemple, une fois que l’on applique un stemming sur « Chiens » ou « Chien », le mot résultant est le même. Cela permet notamment de réduire la taille du vocabulaire dans les approches de type sac de mots ou Tf-IdF.\n",
    "\n",
    "Un des stemmers les plus connus est le Snowball Stemmer. Ce stemmer est disponible en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:49:58.954404Z",
     "start_time": "2023-04-02T22:49:58.937968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           --Stem--            \n",
      "program             program             program             \n",
      "programming         program             program             \n",
      "programer           programer           program             \n",
      "programs            program             program             \n",
      "programmed          program             program             \n"
     ]
    }
   ],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "# Initialize Python porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"--Word--\",\"--Lemma--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}{2:20}\".format(word, wnl.lemmatize(word, pos=\"v\"),ps.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:31:34.576222Z",
     "start_time": "2023-04-02T22:31:34.513508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bouygu', 'a', 'eu', 'une', 'coupur', 'de', 'réseau', 'à', 'marseil', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "def return_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(X.text) for X in doc]\n",
    "\n",
    "return_stem(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:51:29.183939Z",
     "start_time": "2023-04-02T22:51:29.143629Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bouygues', 'a', 'eu', 'une', 'coupure', 'de', 'réseau', 'à', 'Marseille', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def return_lemm(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [nltk.WordNetLemmatizer().lemmatize(X.text) for X in doc]\n",
    "\n",
    "print(return_lemm(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnaissance d'entités nommées (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En traitement automatique du langage, la reconnaissance d’entités nommées cherche à détecter les entités telles que des personnes, des entreprises ou des lieux dans un texte. Cela s’effectue très facilement avec SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:15:21.382024Z",
     "start_time": "2023-04-02T22:15:21.374027Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_NER(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(X.text, X.label_) for X in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:15:25.065616Z",
     "start_time": "2023-04-02T22:15:25.035039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bouygues', 'ORG'), ('Marseille', 'LOC')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_NER(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconnaissances d’entités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy offre des « Visualizers », des outils graphiques qui permettent d’afficher les résultats de reconnaissances d’entités nommées ou d’étiquetage par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:23:47.529184Z",
     "start_time": "2023-04-02T22:15:29.082532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constantinbogdanas/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages/spacy/displacy/__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"fr\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bouygues\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " a eu une coupure de réseau à Marseille.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5001 ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Apr/2023 11:35:52] \"GET / HTTP/1.1\" 200 817\n",
      "127.0.0.1 - - [03/Apr/2023 11:35:53] \"GET /favicon.ico HTTP/1.1\" 200 817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down server on port 5001.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(test)\n",
    "colors = {\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "options = {\"ents\": [\"ORG\"], \"colors\": colors}\n",
    "\n",
    "displacy.serve(doc, style=\"ent\", options=options, port = 5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L’étiquetage morpho-syntaxique ou Part-of-Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’étiquetage morpho-syntaxique ou Part-of-Speech (POS) Tagging en anglais essaye d’attribuer une étiquette à chaque mot d’une phrase mentionnant la fonctionnalité grammaticale d’un mot (Nom propre, adjectif, déterminant…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:23:47.544806Z",
     "start_time": "2023-04-02T22:23:47.529184Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_POS(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(X, X.pos_) for X in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:23:47.589195Z",
     "start_time": "2023-04-02T22:23:47.544806Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Bouygues, 'PROPN'),\n",
       " (a, 'AUX'),\n",
       " (eu, 'VERB'),\n",
       " (une, 'DET'),\n",
       " (coupure, 'NOUN'),\n",
       " (de, 'ADP'),\n",
       " (réseau, 'NOUN'),\n",
       " (à, 'ADP'),\n",
       " (Marseille, 'PROPN'),\n",
       " (., 'PUNCT')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_POS(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dépendances entre ces étiquettes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy dispose également d’une option de visualisation qui nous permet simplement d’afficher les étiquettes identifiées ainsi que les dépendances entre ces étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-02T22:52:08.499Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constantinbogdanas/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages/spacy/displacy/__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"fr\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"df3cb4f550514b65870fee6042b75e57-0\" class=\"displacy\" width=\"1625\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Bouygues</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">eu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">une</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">coupure</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">réseau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">à</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Marseille.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux:tense</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-6\" stroke-width=\"2px\" d=\"M1295,177.0 C1295,89.5 1445.0,89.5 1445.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,179.0 L1287,167.0 1303,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df3cb4f550514b65870fee6042b75e57-0-7\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,2.0 1450.0,2.0 1450.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df3cb4f550514b65870fee6042b75e57-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,179.0 L1458.0,167.0 1442.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5002 ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Apr/2023 11:43:05] \"GET / HTTP/1.1\" 200 7549\n",
      "127.0.0.1 - - [03/Apr/2023 11:43:06] \"GET /favicon.ico HTTP/1.1\" 200 7549\n",
      "127.0.0.1 - - [03/Apr/2023 11:43:29] \"GET / HTTP/1.1\" 200 7549\n",
      "127.0.0.1 - - [03/Apr/2023 11:43:30] \"GET /favicon.ico HTTP/1.1\" 200 7549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down server on port 5002.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(test)\n",
    "displacy.serve(doc, style=\"dep\", port=5002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding par mot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec SpaCy, on peut facilement récupérer le vecteur correspondant à chaque mot une fois passé dans le modèle pré-entraîné en français.\n",
    "\n",
    "Cela nous sert à représenter chaque mot comme étant un vecteur de taille 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.594345Z",
     "start_time": "2023-04-02T22:25:31.578348Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_word_embedding(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(X.tensor) for X in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.690371Z",
     "start_time": "2023-04-02T22:25:31.594345Z"
    }
   },
   "outputs": [],
   "source": [
    "emb = return_word_embedding(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.706374Z",
     "start_time": "2023-04-02T22:25:31.690371Z"
    }
   },
   "source": [
    "Cette information nous sert notamment lorsque l’on cherche à caractériser la similarité entre deux mots ou deux phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarités entre phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de déterminer la similarité entre deux phrases, nous allons opter pour une méthode très simple :\n",
    "\n",
    "déterminer l’embedding moyen d’une phrase en moyennant l’embedding de tous les mots de la phrase\n",
    "calculer la distance entre deux phrases par simple distance euclidienne\n",
    "Cela s’effectue très facilement avec SpaCy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.706374Z",
     "start_time": "2023-04-02T22:25:31.706374Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_mean_embedding(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return np.mean([(X.vector) for X in doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.714385Z",
     "start_time": "2023-04-02T22:25:31.714385Z"
    }
   },
   "outputs": [],
   "source": [
    "test_2 = \"Le réseau sera bientot rétabli à Marseille\"\n",
    "test_3 = \"La panne réseau affecte plusieurs utilisateurs de l'opérateur\"\n",
    "test_4 = \"Il fait 18 degrés ici\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on s’attend à ce que la phrase 2 soit la plus proche de la phrase test, puis la phrase 3 et 4.\n",
    "\n",
    "Avec Numpy, la distance euclidienne se calcule simplement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.718403Z",
     "start_time": "2023-04-02T22:25:31.718403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48619524,  0.15543929,  0.4653287 ,  0.64519817,  0.44107217,\n",
       "        0.49356538,  0.93042785,  0.57736075, -0.6094683 ,  1.8377934 ,\n",
       "        0.6665911 , -1.278511  , -1.0292515 ,  1.4569204 , -1.373152  ,\n",
       "       -1.5997047 ,  0.7098424 , -1.1245949 , -1.5406461 ,  0.15426469,\n",
       "       -1.4622042 ,  3.028283  ,  0.17393643, -1.5314258 ,  0.08824585,\n",
       "       -1.5294561 ,  0.65370035,  0.85241777,  0.29733366, -0.06776382,\n",
       "       -1.412475  , -1.4302574 , -0.2523386 , -1.407089  ,  2.5920694 ,\n",
       "        1.5972013 , -2.466631  ,  0.09732366, -0.57793695, -0.01827459,\n",
       "       -1.094178  ,  0.6201561 , -0.8610061 , -0.39174575,  0.8132372 ,\n",
       "       -0.05847082, -0.8287512 ,  0.14426422, -0.9052164 , -0.10958304,\n",
       "       -0.5539492 , -0.07332305, -1.1061761 , -0.12070229,  2.1586022 ,\n",
       "       -1.3482856 , -1.2779549 ,  0.6481518 ,  0.5812394 ,  0.05964099,\n",
       "        0.2509245 , -0.7327844 , -2.2258992 , -0.6381255 ,  1.7993282 ,\n",
       "        0.6445108 ,  1.6918131 ,  0.8223463 ,  1.034546  ,  0.85517347,\n",
       "       -0.91502875, -0.3412846 ,  0.12411825, -0.40566382,  1.7145336 ,\n",
       "       -0.6540102 , -0.1121001 , -0.7474622 ,  0.6315912 ,  0.50536096,\n",
       "        0.4792098 ,  2.8400855 , -0.5124119 , -1.2319901 ,  0.2968708 ,\n",
       "        1.0678178 , -0.45114666, -0.5237826 , -1.1071619 ,  1.0224383 ,\n",
       "       -0.3025337 , -1.8893324 ,  1.7287161 ,  0.23252027,  1.1227176 ,\n",
       "       -0.8578669 ], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_mean_embedding(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.722416Z",
     "start_time": "2023-04-02T22:25:31.722416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.860769"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(return_mean_embedding(test)-return_mean_embedding(test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.722416Z",
     "start_time": "2023-04-02T22:25:31.722416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.083752"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(return_mean_embedding(test)-return_mean_embedding(test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:25:31.730426Z",
     "start_time": "2023-04-02T22:25:31.730426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.62302"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(return_mean_embedding(test)-return_mean_embedding(test_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La phrase 2 est bien identifiée comme la plus proche, puis la phrase 3 et 4. On peut également utiliser cette approche pour classifier si une phrase appartient à une classe ou à une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
