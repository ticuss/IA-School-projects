{"cells":[{"cell_type":"markdown","source":"## Auto encoders\n\nAuto-encoder and its variants are models that can be used for several things: \n\n- data compression and dimensionality reduction\n- Denoising, recolorisation or super resolution\n- anomaly detection \n\nIn this notebook we will train our first auto-encoder do to MNIST image reconstruction\n","metadata":{"tags":[],"cell_id":"18626afaa7f841ceb7850169c8bb8319","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"###  First auto-encoder without convolutions\n\n\nLoad the mnist dataset into train and test datasets, resize it  normalize them with a minmax scaler","metadata":{"cell_id":"088fbc9af6844353afae949dc144d086","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"f6f87de8544240f3b9e3edb9d373cad8","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Part one the encoder: its goal is to take an original data and reduce its dimension (here to dimension=30)\n\nCreate a first model with (with functional api if keras) with the following layers : \n\n- A dense Relu layer with 300 neurones\n- A dense Relu layer with 30 neurones\n\nName it  encoder\n","metadata":{"cell_id":"3e0d1e5a4f2b42c791c9c7c522f096c1","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"d5c23e76ca8b4a39b606b5912e110b28","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, the decoder. It is the part that takes the compressed data from the encoder and tries to decode it to the original image\n\n\nCreate a second model with the following layers : \n\n- Relu Dense 200\n- Sigmoid Dense 28*28. \n\nWe use sigmoid at the end because MNIST dataset pixels are either black or white. So we can treat that as a classification problem : the output just classify each pixel of the original image\n\n","metadata":{"tags":[],"cell_id":"a374de9661274a70bb651dc47ca39932","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"074926381ada4b59b0254f4118dc8708","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create an auto-encoder model which is the assembly of both the encoder and the decoder","metadata":{"cell_id":"d05672edb3eb4e31af72985655370e2e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"d87b4a5dbe9e47a1bee9b61646db646a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the whole auto-encoder model on the training set. What are the labels that must be used for the .fit method ?","metadata":{"cell_id":"f452ed6f7dd44531b2ce30b7792a5b60","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from tensorflow.keras import Sequential, Input, Model\nfrom tensorflow.keras.layers import Dense \n\n# on fait du mnist comme d'hab ! \n\n\n\ninput = Input((28**2))\n\n\nx = Dense(200, activation=\"relu\")(input)\nx = Dense(30, activation=\"relu\")(x)# the output of this layer is just a 30 dimensional vector \n\nx = Dense(200, activation=\"relu\")(x)\no = Dense(28*28, activation=\"sigmoid\")(x) #car les pixels de MNIST sont noir(0) ou blanc(1) \n\n\n\nauto_encoder = Model(inputs=input, outputs=o)\n","metadata":{"tags":[],"cell_id":"96da1f8469a747daa3440bb458d5651f","source_hash":"6aa76cef","output_cleared":false,"execution_start":1643121813336,"execution_millis":3954,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pareil : x = Dense(30, activation=\"relu\")(x)\n\n## api fonctionnelle : utiliser une couche commme une fonction\nlayer1 = Dense(30, activation=\"relu\")\nprint(type(layer1))\nx = layer1(x)","metadata":{"tags":[],"cell_id":"36f9e353ea3943c4b793009b6ec6f7ff","source_hash":"db502c64","execution_start":1643123975882,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"<class 'tensorflow.python.keras.layers.core.Dense'>\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nfrom tensorflow.keras.datasets.mnist import load_data\n\nmnist = load_data()\ntrain,test = mnist\n\nx_train, y_train = train \n","metadata":{"tags":[],"cell_id":"c1c27159490b4d62aa8f0601536135ba","source_hash":"e8b1e359","execution_start":1643124569155,"execution_millis":331,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ON NE VA PAS UTILISER y_train : on ne fait pas de la classification d'image \n\n\nx_train = x_train.reshape(-1, 28**2) / 255 #(minmax scaling)\n","metadata":{"tags":[],"cell_id":"0c6cd84c451c446aaaaa7d28900f59c8","source_hash":"3ddf6e49","execution_start":1643124573603,"execution_millis":141,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.shape","metadata":{"tags":[],"cell_id":"a8690fa4065d44eb85cd2d96681abbf8","source_hash":"6b4d6ed5","execution_start":1643124574170,"execution_millis":62,"deepnote_output_heights":[21],"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(60000, 784)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"input = Input((28**2,))\n\nx = Dense(200, activation=\"relu\")(input)\nencoder_o = Dense(30, activation=\"relu\")(x)# the output of this layer is just a 30 dimensional vector \n\nx = Dense(200, activation=\"relu\")(encoder_o)\no = Dense(28*28, activation=\"sigmoid\")(x) #car les pixels de MNIST sont noir(0) ou blanc(1) \n\n\nauto_encoder = Model(inputs=input, outputs=o)\n\nencoder =      Model(inputs=input, outputs=encoder_o)\n","metadata":{"tags":[],"cell_id":"a485b8fab2824041bfc97ac34b2eb714","source_hash":"cc4f672f","execution_start":1643126171844,"execution_millis":1,"deepnote_output_heights":[21],"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef apply_decoder(auto_encoder, input):\n\n    o = auto_encoder.layers[-2](input)\n    o = auto_encoder.layers[-1](o)\n    return o\n\ninput_decoder = Input((30,))\ndecoder = Model(input_decoder, apply_decoder(auto_encoder, input_decoder))\n\n\n","metadata":{"tags":[],"cell_id":"da80578b911449a1befe486248e58d57","source_hash":"20c62cca","execution_start":1643126136511,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder.predict(x_train[:3]).shape","metadata":{"tags":[],"cell_id":"83cc70e36cfb46b0a00c8bf5dcec5352","source_hash":"3cc2eab4","execution_start":1643125162655,"execution_millis":167,"deepnote_output_heights":[21],"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"(3, 30)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"a4dff7333c6c462191e1c4abf26c48ce","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nauto_encoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\") # l'accuracy n'est pas une métrique pertinente ici\nauto_encoder.fit(x_train, x_train, epochs=10)","metadata":{"tags":[],"cell_id":"27fc3767e71c4f7a8a5035cf5db8f0de","source_hash":"5fa5ade3","execution_start":1643125114210,"execution_millis":17066,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/10\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.1748\nEpoch 2/10\n 671/1875 [=========>....................] - ETA: 7s - loss: 0.0988","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-bb6912160285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# l'accuracy n'est pas une métrique pertinente ici\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":null},{"cell_type":"code","source":"X_pred = auto_encoder.predict(x_train[:3])\n\nimport matplotlib.pyplot as plt \n\nfirst_pred = X_pred[0]\nprint(first_pred.shape)\nplt.imshow(first_pred.reshape(28, 28), cmap=\"gray\")\nplt.figure()\nplt.imshow(x_train[0].reshape(28, 28), cmap=\"gray\")","metadata":{"tags":[],"cell_id":"a9b6ab5c1c5f4389ade1aff687baf144","source_hash":"6bfa3902","execution_start":1643124726386,"execution_millis":888,"deepnote_output_heights":[null,21,250,250],"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"(784,)\n","output_type":"stream"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f1fd7bfd3d0>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light","image/png":{"width":251,"height":248}},"output_type":"display_data"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light","image/png":{"width":251,"height":248}},"output_type":"display_data"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow\nlayer = tensorflow.keras.layers.UpSampling2D(\n    size=(2, 2), data_format=None, interpolation=\"nearest\"\n)\n\nx = Input((2, 2, 1))\n\ny = layer(x)\nprint(y.shape)","metadata":{"tags":[],"cell_id":"b0dade250ba4497d9b02183a73de4a68","source_hash":"ec254d5e","execution_start":1643125490677,"execution_millis":21,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"(None, 4, 4, 1)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n\nX = train \ny = X\n\nauto_encoder.fit(X, X)\n\nresult = auto_encoder.predict(X[0]) => resortira un vecteur \nplt.imshow(result.reshape(28, 28), cmap=\"gray\")","metadata":{"tags":[],"cell_id":"5b8a0039c4304e9aba50fa839de20aae","source_hash":"c6e2ced1","execution_start":1643117087692,"execution_millis":59,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_2\" was not an Input tensor, it was generated by layer dense_1.\nNote that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\nThe tensor that caused the issue was: dense_1/Relu:0\n","output_type":"stream"},{"output_type":"error","ename":"ValueError","evalue":"Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e9b4997323ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#car les pixels de MNIST sont noir(0) ou blanc(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mauto_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 204\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    988\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []"]}],"execution_count":null},{"cell_type":"markdown","source":"Feed some images to the model and display the reconstructed images along with the original images. \n\n","metadata":{"cell_id":"15cc20d0a1fb4e0792aae3f960a7c3e2","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def reconstruction_error(X, X_pred):\n\n    return mse(X.reshape(-1, 1), X.reshape(-1, 1))\n\n\ndef is_anomaly(model, X, threshold):\n    X_pred = model.predict(X)\n\n    return (reconstruction_error > threshold)\n        ","metadata":{"cell_id":"df7c1ffb63ee4ab0bfc2afac1081eaa5","source_hash":"54fdbf98","output_cleared":false,"execution_start":1607341917454,"execution_millis":423,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN based Auto-encoder\n\nWe can of course have auto-encoder with convolutions ! \n\nWe will replace\n\n- Dense layers by convolution layers in the encoder model and add some maxPooling after each convolution\n- Dense layers by Upsampling2D layers in the decoder model\n\n","metadata":{"cell_id":"383d4e9f95b343768eaf18a48d4f15b9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"1. Create an Upsampling2D layer with Keras\n\n2. Calculate the output of the layer and store the result in the upsampled variable on the array created above.\n\nReacreate the encoder, decoder and auto-coder model as previously","metadata":{"cell_id":"7655aa19a35c4637918346d7ce6f1970","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"f150ffa6f6734ce994e968d56cb0bb68","source_hash":"572a2d1f","output_cleared":false,"execution_start":1607341930915,"execution_millis":1,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reload the MNIST dataset and do the proper scaling and resizing (rember, keras convolutions expects to see a 4D array with the dimensions (nb_exemple, n_col, n_row, n_channel)","metadata":{"cell_id":"48d0ba75b82445cfa4d5aa789660b00b","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"5cbd4289fe834b7c86e6c1244cbd3063","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the auto-encoder model. Display some reconstructed images. Does it work better than the previous model?","metadata":{"cell_id":"4d0e8ab0e18d4b04bfcfd7b32cbe1160","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"66e9c61661dc4e2a80c6a8db844b5d04","source_hash":"ff241c96","output_cleared":false,"execution_start":1607341930916,"execution_millis":68,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : Denoising auto-encoder. \n\nLet's say that we have two version of our images : \n\n- image with noise\n- the same image without noise. \n\nWe can ask our auto-encoder to reconstruct clean images from noisy images\n\n","metadata":{"cell_id":"f20bf3a9de2a41e4a27ae46be87ae818","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Load the Mnist dataset. \n","metadata":{"cell_id":"58a2a2646ebe46569001b0605bebed55","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"5f40659f230346ffae90ed4f006c3fd7","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the noise function bellow, create a corrupted version of MNIST by adding some noise","metadata":{"cell_id":"7a2b0db897f045268631b8146ded43d4","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def noise(array):\n    \"\"\"\n    Adds random noise to each image in the supplied array.\n    \"\"\"\n\n    noise_factor = 0.4\n    noisy_array = array + noise_factor * np.random.normal(\n        loc=0.0, scale=1.0, size=array.shape\n    )\n\n    return np.clip(noisy_array, 0.0, 1.0)","metadata":{"cell_id":"6af2d04e6d6a45dcafbffda3d6f32fc1","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display some noisy images and check you can still recognize mnist dataset ","metadata":{"cell_id":"0667c23276314a4f80a87bbc06347deb","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"cb222a60dd924f22872e686b9b869610","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instanciate a new version of the CNN auto-encoder. Train it now to reconstruct clean images from noisy ones. ","metadata":{"cell_id":"3631deae55364d3781f18b8aeee4f200","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"fed2164c98e54ea5abbd597d11e26bce","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feed some noisy image to the network and check that it outputs some cleaned images with matplotlib","metadata":{"cell_id":"5814ec7a8575446b959706e1cf68c06d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"0164528dbfac48b2acdd130c4bb7b09a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Search and Think about other applications of auto-encoder ","metadata":{"cell_id":"4d5b425d80aa4a429b60d1ab464d392a","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"574ab83aba2148cb867c15b92eae8f71","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6b859965-b858-4b8d-a841-009599aef86e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"cb4b55bc8feb4999ae0356806f8cad39","deepnote_execution_queue":[]}}