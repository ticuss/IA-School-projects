{"cells":[{"cell_type":"markdown","source":"## Neural network - basic behavior\n\nLaurent Cetinsoy - Datadidacte - personal use only\n\nIn order to get a bit of understanding and practice with keras we are going to test a few different architectures and their impact of the model performance","metadata":{"cell_id":"b9ea161477404cca975cf1bc089e7677","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Effect of the activation function\n\nwe want to see what is the impact of the activation function and compare the performance of sigmoid layers versus relu layers","metadata":{"cell_id":"bfaee740e7eb441291293c62b2bf8ccb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Load the  mnist dataset from the tensorflow.keras.datasets module ","metadata":{"cell_id":"61f55694f7b64cc8ba85a609479c622a","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"ee7fd4d49d69438588ecd76c84cea50d","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the first image of the train dataset with matplotlib","metadata":{"cell_id":"09444dffecb3494e8fcce4711d75e251","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"3120d12a3d5e45d9bd76219ffbc05434","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the number of classes in the dataset","metadata":{"cell_id":"266157bccc8c48349ed5ed812052e2b1","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"a368eba2f6d6492db76ec1fd338b6cae","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a simple neural network with the following structure : \n\n- One dense layer with **sigmoid** activation and 300 neurones\n- One dense layer with **sigmoid** activation and 100 neurones \n- One dense layer with softmax activation. How many neurones should there be ? ","metadata":{"cell_id":"8361db6b8bdd4fb08061ce8455238711","deepnote_cell_height":152.8333282470703,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"11752e5dc26c44f99edf6e2eaf458427","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compile the model with the sgd optimizer and the sparse_categorical_crossentropy_loss","metadata":{"cell_id":"5298f428bb3b4383a5e522351b9cfddc","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"2bf1b2065c124a3eba1e8ac0cf63a165","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reshape the images so that each image is a vector. Remember to normalize the dataset by doing a min-max scaling\n","metadata":{"cell_id":"9d2b0c34650d452981eba5e3e9d6889e","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"445a36b0c625406191334edab48aac42","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the model for 10 epochs and display the loss versus the number of epochs. For that you can use Keras callbacks : https://keras.io/api/callbacks/\n\nDoes it train well ? ","metadata":{"cell_id":"6b21caa6a2764e879e3795dd3b59e7e4","deepnote_cell_height":110.43333435058594,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"30fe5b66b50f4ec0bf7e19d6e8867a59","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Make another model by replacing this time the sigmoid activations by Relu. Train again the model and display the loss curve. Is it better ? ","metadata":{"cell_id":"dd1d0127edd745a9889838db1b608754","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"c0be5b9ee9ed4629a7597108809f7df1","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Do the same with the selu function ","metadata":{"cell_id":"cb4f878776564f1fbefdd2daad27eaf5","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"fd30dfce52674b3fbff9e5aae7228a71","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Effect of the size \n\nIn this section we want to study the effect of the size of the network on the learning\n\nRe-use the previous network and try to increase its size by using the following number of neurones : \n\n-  800 for first layer\n- 500 for second layer\n- same size\n\n\nYou might need to train the network longer, but does it reach a better performance ? ","metadata":{"cell_id":"9fbfef0e5ca54b298b58a12e3cd4935d","deepnote_cell_height":290.0333251953125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"cd9ae20ff1f044a6b4965fc8df1f8f71","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : train a big network (more than 5 layers). Does it overfit or has it still good performance ? ","metadata":{"cell_id":"b686eb655603419e8a66b0bb173fc517","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"5498d0f3498e4e8c94849aac6b33fc07","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Effect of the parameter initialization \n\nThe way the parameters of a network are initialized can have a great impact on the training process. \n\n","metadata":{"cell_id":"81466cec36d04aef84554c6c5bb54d96","deepnote_cell_height":130.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Create a model with the same structure as the previous section. Initialize all weight values to 0 (cf https://keras.io/api/layers/initializers/)","metadata":{"cell_id":"2eec27b1be354bfda39202a12a4bf13c","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"64de9d5ffccf4f8189173b0a2e8cbaee","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the model on the dataset and display the loss. Are you shocked ? ","metadata":{"cell_id":"76a8ae13768446c9b03ec43e7042d1bb","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"7c9f5266e6384ca89a5d8c1fd22c290d","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Effect of the optimizer\n\nThis time we want to study the effect of the optimization process on the network \n\n\nInstanciate the Optimizer SGD with a learning rate equaling to 0.1 and train the model ","metadata":{"cell_id":"25a79b681b074133a3fbc61cfe609c9e","deepnote_cell_height":144.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"a0b053b171624541b9568cfd9cfabffb","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Do the same with a learning rate of 0.01","metadata":{"cell_id":"138b0c627153480ca429e0826f5d3673","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"5456d5181b824ec88ac73f0f855b3b73","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Do the same with a learning rate of 0.9. Does it still work ? ","metadata":{"cell_id":"d3a1177cbf754e36b35f500dabb468d0","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"84f8450d14ad41ffa7c8eecb13449029","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the callback Reduce learning rate on plateau (https://keras.io/api/callbacks/reduce_lr_on_plateau/) and start with a pretty high learning rate. Is the learning faster ?\n","metadata":{"cell_id":"5edaadfba26e45ae84bf62d2bf50e18f","deepnote_cell_height":96.43333435058594,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"6be6bf63b39444d3a785a2b05f5d99cb","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : with the https://keras.io/api/callbacks/learning_rate_scheduler/ \n\nImplement the cyclical learning rate schema as follow (from \"Cyclical Learning Rates for Training Neural Networks\" Smith - https://arxiv.org/abs/1506.01186)","metadata":{"cell_id":"6a083a8d343943ce80133998c6f8fe4c","deepnote_cell_height":110.43333435058594,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"97f9afd126da4b8e858f3dfc01d9faf5","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : do the study activation function and size on a more complexe dataset like cifar 10 ","metadata":{"tags":[],"cell_id":"b7bf079fb1d94aa2bc04e6804a8136ce","deepnote_cell_height":74.03334045410156,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"c1f3fe19b521456894411fdc78d5ae76","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : do the same with a small convolutional network","metadata":{"tags":[],"cell_id":"102ae00fa3d94dcbb967ed3c83f270dd","deepnote_cell_height":51.633331298828125,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"06c2298dfb274eb3b308762839ef934e","deepnote_cell_height":65.23333740234375,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"73611d27e6434a77a3d9a399fc17b75b","deepnote_cell_height":45.23333740234375,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6b859965-b858-4b8d-a841-009599aef86e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"f929cf570a4f426ab84465c202dbe36f","deepnote_execution_queue":[]}}