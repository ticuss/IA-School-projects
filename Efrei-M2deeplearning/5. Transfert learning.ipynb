{"cells":[{"cell_type":"markdown","source":"## Transfert learning \n\nTransfert learning is a method consisting in re-using a trained model. The trained model will be retrained on the target dataset. \n\nFor instance, let's say you want to train  classification tasks about cars. You could take a neural network already trained on generic image classification and train it again on your dataset.\n\nThe basic idea is that it will be quicker to train a \"competent\" network in a similar task than from starting from an empty \"brain\". \n\nIn this notebook we will use this method and try to see the various methods\n","metadata":{"tags":[],"cell_id":"64170f0cec12497cb73de9d185badaf8","deepnote_cell_height":284.26666259765625,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Keras has various pretrained models available at https://keras.io/api/applications/\n\nWe will use theses pretrained model as it they were a simple layer. \n\n\nWe will start with mobilenet\n\n","metadata":{"tags":[],"cell_id":"529bf22667f94a65b77d0bc5bc4a35ec","deepnote_cell_height":124.66667175292969,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"From the **keras.applications module**, load the the mobile net network into a variable named **base_model**\n\nUse include_top = False so that the Dense layers are removed","metadata":{"tags":[],"cell_id":"91d0d01d582440919b345cc35ab59759","deepnote_cell_height":88.26666259765625,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"2a5ae43397de43f699a15c1d9f8d7fb1","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the number of layer of this base model","metadata":{"tags":[],"cell_id":"5221f25a62c548248e27db529634aff1","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"624720e8197d410f8e4683ab5911d696","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display all the layer of the model. You should see only convolutional layers","metadata":{"tags":[],"cell_id":"da4fed0883ca40009ea8fc22a1c3d036","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"f802874ec0dd40cfbaceb90a128aab03","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nmodel.layers contains all the model layers and their configuration. \n\nEach layer is an instance of Check https://keras.io/api/layers/base_layer/#layer-class and\n\ntell what are the attributes of the base layer classes by looking at the documentation\n","metadata":{"tags":[],"cell_id":"728bb4cc5f564be39f505a81342d4f5b","deepnote_cell_height":124.66667175292969,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"061eda179b1d4ce8acdaacec005daa4c","deepnote_cell_height":45.46665954589844,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\nYou may have seen an attribute trainable. It tells if the layer weights can be updated or not. \n\n\nLoop over the layers of the model and display if the layers are trainable or not","metadata":{"tags":[],"cell_id":"6162afb3d495465c8d17f28ef3ebdc7c","deepnote_cell_height":88.26666259765625,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"dd118f627d5147e9b65ddc8917481492","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update the last convolutional layer et make it not trainable","metadata":{"tags":[],"cell_id":"9489dd785c354932b709c29823119567","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"5f2f06d561ec4fd78d913971b12f3f15","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we want to add our own dense layers on top of the model. \nWe have two ways of doing that : either using the sequential api or the functional one \n\n\nCreate a model using the base model and two Dense layer (400 and 200 neurones ) + the final layer to do classification ","metadata":{"tags":[],"cell_id":"3eaae58bb06c4f20a5ced2619de0107b","deepnote_cell_height":110.66667175292969,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"918ce7b10b354df58f62f7cd33a37b42","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the cifar dataset, and create a x_train, and y_train variable from it","metadata":{"tags":[],"cell_id":"1fd1476733ad4e779f11f8b047818cad","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"ea40067d2bfb41b693f56bb3a28573d6","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check if you need to standardise x_train and do it if you need","metadata":{"tags":[],"cell_id":"7bc81e9f24e147958371a9e953580eee","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"3e067634a8cb4a748868f11b1286c162","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the transfert model \n\nWe can think of several methods to train the transfert model :  \n\n- train the whole model\n- train only the added layers (Dense ones) \n- A mix of both. \n\nWe will try to see which one works\n\n### Training the whole model  \n","metadata":{"tags":[],"cell_id":"60139612cbae4098ad3a65cc861cd86a","deepnote_cell_height":278.566650390625,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"By looping over all layers, make sur all layers are trainable\n","metadata":{"tags":[],"cell_id":"4b0bc1078efe4da0b0f61b9b07978b26","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"45259ab8213744bb9f1f1b8728be12a6","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train your model, does it work ? \n","metadata":{"tags":[],"cell_id":"96e855bd4fe94d0bb018aeb4fb8c2da6","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"67386b24e711468fbc2da03a23a5edfa","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training only the last layers\n\nDo the same thing as before and but make the last layer of the base model trainable","metadata":{"tags":[],"cell_id":"e45291e1ec68410c8fcc651b7b87a707","deepnote_cell_height":99.86666870117188,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"d45eeafb369c43caa637b1dc1d878e2f","owner_user_id":"6493ea36-0370-4fcc-a539-2fe0671931ca","deepnote_cell_height":65,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### A mix of both\n\nWe have seen that if we train the whole model it does not work. But how can we train the convolutional layers ? \n\n\nOne option is to do as follow : \n- train the dense layer only first\n- then train the convolutional layers\n\nthis methode is called finetuning","metadata":{"tags":[],"cell_id":"845e0b728c624c9dad6fc349ac5a005e","deepnote_cell_height":235.1999969482422,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Make only the last layers trainable and train it","metadata":{"tags":[],"cell_id":"60c244ae52eb493d9df72085cdcf30d0","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"c2faf37f9789409faaa3dd3ccc411dbe","deepnote_cell_height":65.46665954589844,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unfreeze the last 3 convolutional layers and make them trainable. Train the model again","metadata":{"tags":[],"cell_id":"5689e226ebf74a3b98bbdb068202194b","deepnote_cell_height":51.866668701171875,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"4e04f140f0044ff9926ca4f52eb5cf32","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6b859965-b858-4b8d-a841-009599aef86e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{"is_reactive":false},"orig_nbformat":2,"deepnote_notebook_id":"5e951b46f166414e8c1d806ba65144f4","deepnote_execution_queue":[]}}