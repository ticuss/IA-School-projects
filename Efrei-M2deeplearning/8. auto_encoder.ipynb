{"cells":[{"cell_type":"markdown","source":"## Auto encoders\n\nAuto-encoder and its variants are models that can be used for several things: \n\n- data compression and dimensionality reduction\n- Denoising, recolorisation or super resolution\n- anomaly detection \n\nIn this notebook we will train our first auto-encoder do to MNIST image reconstruction\n","metadata":{"tags":[],"cell_id":"30022df6efd945038c992ebc472a2b6a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"###  First auto-encoder without convolutions\n\n\nLoad the mnist dataset into train and test datasets, resize it  normalize them with a minmax scaler","metadata":{"cell_id":"cb9983a9f6724677a934d1aaafa0fc8b","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"dc2b193cd3e048ff9e0250f6781d1f5a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Part one the encoder: its goal is to take an original data and reduce its dimension (here to dimension=30)\n\nCreate a first model with (with functional api if keras) with the following layers : \n\n- A dense Relu layer with 300 neurones\n- A dense Relu layer with 30 neurones\n\nName it  encoder\n","metadata":{"cell_id":"713fcf7489f741e8965eeab8bcd3689f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"97ed39ea36f6443392adaa351364346e","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, the decoder. It is the part that takes the compressed data from the encoder and tries to decode it to the original image\n\n\nCreate a second model with the following layers : \n\n- Relu Dense 200\n- Sigmoid Dense 28*28. \n\nWe use sigmoid at the end because MNIST dataset pixels are either black or white. So we can treat that as a classification problem : the output just classify each pixel of the original image\n\n","metadata":{"tags":[],"cell_id":"aae4b1e4e32b47d6a31a0dbdfe304f3d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"cc8c7e9bdacf42588707b2b3ab54853e","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create an auto-encoder model which is the assembly of both the encoder and the decoder","metadata":{"cell_id":"88d1d3121ff2436792b5b427063d9c15","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"41bcebd370dc496484237003d5fb9fe9","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the whole auto-encoder model on the training set. What are the labels that must be used for the .fit method ?","metadata":{"cell_id":"233db18b0f6248c78ca418079d390222","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"cfbbe0cd42f04a589c2623f1ad742c8d","source_hash":"b623e53d","output_cleared":false,"execution_start":1607341917450,"execution_millis":2,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feed some images to the model and display the reconstructed images along with the original images. \n\n","metadata":{"cell_id":"52d46baafefd4058b95b8a2548f24d78","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"0b1fc334b3714df3ab0117596c91b7c2","source_hash":"54fdbf98","output_cleared":false,"execution_start":1607341917454,"execution_millis":423,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN based Auto-encoder\n\nWe can of course have auto-encoder with convolutions ! \n\nWe will replace\n\n- Dense layers by convolution layers in the encoder model and add some maxPooling after each convolution\n- Dense layers by Upsampling2D layers in the decoder model\n\n","metadata":{"cell_id":"9126226f85bd4c0aa4536a045f7bda83","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"1. Create an Upsampling2D layer with Keras\n\n2. Calculate the output of the layer and store the result in the upsampled variable on the array created above.\n\nReacreate the encoder, decoder and auto-coder model as previously","metadata":{"cell_id":"ad29c46870b847d081d854d29cc4b0b2","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"fdc2628b5117479b9eec20a1f8db2df7","source_hash":"572a2d1f","output_cleared":false,"execution_start":1607341930915,"execution_millis":1,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reload the MNIST dataset and do the proper scaling and resizing (rember, keras convolutions expects to see a 4D array with the dimensions (nb_exemple, n_col, n_row, n_channel)","metadata":{"cell_id":"bfb245bbb8644b31aff21525d5099406","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"9dd6bc8032384fa697379404984f243c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the auto-encoder model. Display some reconstructed images. Does it work better than the previous model?","metadata":{"cell_id":"5eea9e21e43d4cd3b2d991da0dcecf98","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"efd9f521c664498985fe5842f6bdd409","source_hash":"ff241c96","output_cleared":false,"execution_start":1607341930916,"execution_millis":68,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bonus : Denoising auto-encoder. \n\nLet's say that we have two version of our images : \n\n- image with noise\n- the same image without noise. \n\nWe can ask our auto-encoder to reconstruct clean images from noisy images\n\n","metadata":{"cell_id":"6d42c573e7da4e7c8fce8d70656fd42b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Load the Mnist dataset. \n","metadata":{"cell_id":"2a4b419c6b8c485db33931ad9048c1fa","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"7f619f25fcf04423a9b712ca47aff575","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the noise function bellow, create a corrupted version of MNIST by adding some noise","metadata":{"cell_id":"fbe43efda09348f190879bf614a5a958","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def noise(array):\n    \"\"\"\n    Adds random noise to each image in the supplied array.\n    \"\"\"\n\n    noise_factor = 0.4\n    noisy_array = array + noise_factor * np.random.normal(\n        loc=0.0, scale=1.0, size=array.shape\n    )\n\n    return np.clip(noisy_array, 0.0, 1.0)","metadata":{"cell_id":"bab3ea0e2d28451086cc9ca93e7c2315","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display some noisy images and check you can still recognize mnist dataset ","metadata":{"cell_id":"9d7a46e9fcbe4f99a30935b7e5a3e1f9","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"37d934c692c74143a2c34489046e6b9b","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instanciate a new version of the CNN auto-encoder. Train it now to reconstruct clean images from noisy ones. ","metadata":{"cell_id":"60ddcdfff0f141c590a9ff56fc334a50","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"2eb65950dc20422c93dfeaaded3ecce3","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feed some noisy image to the network and check that it outputs some cleaned images with matplotlib","metadata":{"cell_id":"296951374f5143299b8cfcbead4aea9c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"244aa67371a54948a37308959b4841d7","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Search and Think about other applications of auto-encoder ","metadata":{"cell_id":"927a38d7469746f985ebe56f7c3703f1","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"cell_id":"3acc99c533714ac1a13d8c049ec874bd","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6b859965-b858-4b8d-a841-009599aef86e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"59684c622c39440999d3396ce18ac281","deepnote_execution_queue":[]}}